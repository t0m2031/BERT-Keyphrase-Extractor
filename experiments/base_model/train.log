2020-06-19 16:41:27,396:INFO: device: cpu, n_gpu: 0, 16-bits training: False
2020-06-19 16:41:27,396:INFO: Loading the datasets...
2020-06-19 16:41:27,396:ERROR: Model name 'model/' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese). We assumed 'model/' was a path or url but couldn't find any file associated to this path or url.
2020-06-19 16:42:43,305:INFO: device: cpu, n_gpu: 0, 16-bits training: False
2020-06-19 16:42:43,305:INFO: Loading the datasets...
2020-06-19 16:42:43,305:INFO: loading vocabulary file model/vocab.txt
2020-06-19 16:45:04,026:INFO: device: cpu, n_gpu: 0, 16-bits training: False
2020-06-19 16:45:04,026:INFO: Loading the datasets...
2020-06-19 16:45:04,026:INFO: loading vocabulary file model/vocab.txt
2020-06-19 16:45:04,311:INFO: loading archive file model/
2020-06-19 16:49:19,540:INFO: device: cpu, n_gpu: 0, 16-bits training: False
2020-06-19 16:49:19,540:INFO: Loading the datasets...
2020-06-19 16:49:19,540:INFO: loading vocabulary file model/vocab.txt
2020-06-19 16:49:19,609:INFO: loading archive file model/
2020-06-19 16:49:19,609:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 31090
}

2020-06-19 16:49:23,208:INFO: Weights of BertForTokenClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2020-06-19 16:49:23,208:INFO: Weights from pretrained model not used in BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2020-06-19 16:49:23,216:INFO: Starting training for 6 epoch(s)
2020-06-19 16:49:23,216:INFO: Epoch 1/6
2020-06-19 17:11:06,189:INFO: - Train metrics: loss: 00.18; f1: 44.90
2020-06-19 17:13:16,798:INFO: - Val metrics: loss: 00.32; f1: 34.45
2020-06-19 17:13:59,568:INFO: - Found new best F1
2020-06-19 17:13:59,568:INFO: Epoch 2/6
2020-06-19 17:36:20,351:INFO: - Train metrics: loss: 00.10; f1: 57.14
2020-06-19 17:38:26,877:INFO: - Val metrics: loss: 00.34; f1: 38.41
2020-06-19 17:39:03,858:INFO: - Found new best F1
2020-06-19 17:39:03,944:INFO: Epoch 3/6
2020-06-19 18:13:34,005:INFO: - Train metrics: loss: 00.05; f1: 65.35
2020-06-19 18:15:41,646:INFO: - Val metrics: loss: 00.28; f1: 43.80
2020-06-19 18:16:23,138:INFO: - Found new best F1
2020-06-19 18:16:23,187:INFO: Epoch 4/6
2020-06-19 18:38:23,440:INFO: - Train metrics: loss: 00.03; f1: 69.24
2020-06-19 18:40:30,446:INFO: - Val metrics: loss: 00.41; f1: 39.63
2020-06-19 18:40:49,135:INFO: Epoch 5/6
2020-06-19 19:02:52,625:INFO: - Train metrics: loss: 00.02; f1: 71.66
2020-06-19 19:05:06,498:INFO: - Val metrics: loss: 00.39; f1: 43.46
2020-06-19 19:05:21,795:INFO: Epoch 6/6
2020-06-19 19:26:48,104:INFO: - Train metrics: loss: 00.01; f1: 72.34
2020-06-19 19:28:58,098:INFO: - Val metrics: loss: 00.41; f1: 43.56
2020-06-19 19:29:14,370:INFO: Best val f1: 43.80
2020-06-19 19:59:45,477:INFO: device: cpu, n_gpu: 0, 16-bits training: False
2020-06-19 19:59:45,493:INFO: Loading the datasets...
2020-06-19 19:59:45,493:INFO: loading vocabulary file model/vocab.txt
2020-06-19 20:08:22,400:INFO: device: cpu, n_gpu: 0, 16-bits training: False
2020-06-19 20:08:22,400:INFO: Loading the datasets...
2020-06-19 20:08:22,402:INFO: loading vocabulary file model/vocab.txt
2020-06-19 20:11:42,385:INFO: device: cpu, n_gpu: 0, 16-bits training: False
2020-06-19 20:11:42,385:INFO: Loading the datasets...
2020-06-19 20:11:42,386:INFO: loading vocabulary file model/vocab.txt
2020-06-19 20:22:45,431:INFO: device: cpu, n_gpu: 0, 16-bits training: False
2020-06-19 20:22:45,432:INFO: Loading the datasets...
2020-06-19 20:22:45,478:INFO: loading vocabulary file model/vocab.txt
2020-06-19 20:29:23,614:INFO: device: cpu, n_gpu: 0, 16-bits training: False
2020-06-19 20:29:23,614:INFO: Loading the datasets...
2020-06-19 20:29:58,982:INFO: device: cpu, n_gpu: 0, 16-bits training: False
2020-06-19 20:29:58,983:INFO: Loading the datasets...
2020-06-19 20:30:01,086:INFO: loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\Users\SSG\.pytorch_pretrained_bert\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
2020-06-19 20:43:58,280:INFO: device: cpu, n_gpu: 0, 16-bits training: False
2020-06-19 20:43:58,280:INFO: Loading the datasets...
2020-06-19 20:43:59,858:INFO: loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\Users\SSG\.pytorch_pretrained_bert\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
2020-06-19 20:48:23,253:INFO: device: cpu, n_gpu: 0, 16-bits training: False
2020-06-19 20:48:23,253:INFO: Loading the datasets...
2020-06-19 20:48:24,824:INFO: loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\Users\SSG\.pytorch_pretrained_bert\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
2020-06-19 21:47:19,631:INFO: device: cpu, n_gpu: 0, 16-bits training: False
2020-06-19 21:47:19,631:INFO: Loading the datasets...
2020-06-19 21:47:21,044:INFO: Lock 1937423582536 acquired on C:\Users\SSG/.cache\torch\transformers\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock
2020-06-19 21:47:21,044:INFO: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to C:\Users\SSG\.cache\torch\transformers\tmpj9wpdt6u
2020-06-19 21:47:24,433:INFO: storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at C:\Users\SSG/.cache\torch\transformers\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
2020-06-19 21:47:24,433:INFO: creating metadata file for C:\Users\SSG/.cache\torch\transformers\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
2020-06-19 21:47:24,433:INFO: Lock 1937423582536 released on C:\Users\SSG/.cache\torch\transformers\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock
2020-06-19 21:47:24,433:INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\Users\SSG/.cache\torch\transformers\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
2020-06-19 21:47:25,350:INFO: loading archive file model/
2020-06-19 21:47:25,350:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 31090
}

2020-06-19 21:47:33,157:INFO: Weights of BertForTokenClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2020-06-19 21:47:33,157:INFO: Weights from pretrained model not used in BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2020-06-19 21:47:33,165:INFO: Starting training for 6 epoch(s)
2020-06-19 21:47:33,165:INFO: Epoch 1/6
2020-06-19 22:09:24,629:INFO: device: cpu, n_gpu: 0, 16-bits training: False
2020-06-19 22:09:24,629:INFO: Loading the datasets...
2020-06-19 22:09:27,227:INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\Users\SSG/.cache\torch\transformers\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
2020-06-19 22:09:28,048:INFO: loading archive file model/
2020-06-19 22:09:28,064:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-06-19 22:11:23,262:INFO: device: cpu, n_gpu: 0, 16-bits training: False
2020-06-19 22:11:23,270:INFO: Loading the datasets...
2020-06-19 22:11:25,564:INFO: loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\Users\SSG\.pytorch_pretrained_bert\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
2020-06-19 22:11:45,369:INFO: device: cpu, n_gpu: 0, 16-bits training: False
2020-06-19 22:11:45,369:INFO: Loading the datasets...
2020-06-19 22:11:47,274:INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\Users\SSG/.cache\torch\transformers\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
2020-06-19 22:11:47,370:INFO: loading archive file model/
2020-06-19 22:11:47,370:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-06-19 22:19:10,426:INFO: device: cpu, n_gpu: 0, 16-bits training: False
2020-06-19 22:19:10,426:INFO: Loading the datasets...
2020-06-19 22:19:12,938:INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\Users\SSG/.cache\torch\transformers\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
2020-06-19 22:19:13,058:INFO: loading archive file model/
2020-06-19 22:19:13,066:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-06-19 22:19:15,770:INFO: Weights of BertForTokenClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2020-06-19 22:19:15,770:INFO: Weights from pretrained model not used in BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']
2020-06-19 22:26:52,236:INFO: device: cpu, n_gpu: 0, 16-bits training: False
2020-06-19 22:26:52,236:INFO: Loading the datasets...
2020-06-19 22:26:54,328:INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\Users\SSG/.cache\torch\transformers\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
2020-06-19 22:26:54,432:INFO: loading archive file model/
2020-06-19 22:26:54,433:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 31090
}

2020-06-19 22:26:57,553:INFO: Weights of BertForTokenClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2020-06-19 22:26:57,553:INFO: Weights from pretrained model not used in BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2020-06-19 22:26:57,689:INFO: Starting training for 6 epoch(s)
2020-06-19 22:26:57,689:INFO: Epoch 1/6
2020-06-19 22:46:40,178:INFO: - Train metrics: loss: 00.36; f1: 19.85
2020-06-19 22:48:36,432:INFO: - Val metrics: loss: 00.44; f1: 17.68
2020-06-19 22:49:12,779:INFO: - Found new best F1
2020-06-19 22:49:12,780:INFO: Epoch 2/6
2020-06-19 23:08:40,050:INFO: - Train metrics: loss: 00.26; f1: 26.23
2020-06-19 23:10:34,395:INFO: - Val metrics: loss: 00.43; f1: 17.08
2020-06-19 23:10:48,657:INFO: Epoch 3/6
2020-06-19 23:30:20,675:INFO: - Train metrics: loss: 00.20; f1: 33.61
2020-06-19 23:32:19,129:INFO: - Val metrics: loss: 00.43; f1: 18.22
2020-06-19 23:32:53,007:INFO: - Found new best F1
2020-06-19 23:32:53,009:INFO: Epoch 4/6
2020-06-19 23:52:53,603:INFO: - Train metrics: loss: 00.17; f1: 39.27
2020-06-19 23:54:48,516:INFO: - Val metrics: loss: 00.59; f1: 17.96
2020-06-19 23:54:58,759:INFO: Epoch 5/6
2020-06-20 00:15:16,631:INFO: - Train metrics: loss: 00.11; f1: 48.92
2020-06-20 00:17:20,652:INFO: - Val metrics: loss: 00.56; f1: 19.22
2020-06-20 00:18:08,208:INFO: - Found new best F1
2020-06-20 00:18:08,208:INFO: Epoch 6/6
2020-06-20 00:38:55,734:INFO: - Train metrics: loss: 00.08; f1: 55.42
2020-06-20 00:40:55,221:INFO: - Val metrics: loss: 00.51; f1: 20.95
2020-06-20 00:41:37,806:INFO: - Found new best F1
2020-06-20 00:41:37,814:INFO: Best val f1: 20.95
2020-06-20 11:00:32,243:INFO: device: cpu, n_gpu: 0, 16-bits training: False
2020-06-20 11:00:32,244:INFO: Loading the datasets...
2020-06-20 11:00:34,148:INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\Users\SSG/.cache\torch\transformers\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
2020-06-20 11:00:34,453:INFO: loading archive file model/
2020-06-20 11:00:34,454:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 31090
}

2020-06-20 11:00:42,179:INFO: Weights of BertForTokenClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2020-06-20 11:00:42,179:INFO: Weights from pretrained model not used in BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2020-06-20 11:00:42,189:INFO: Starting training for 4 epoch(s)
2020-06-20 11:00:42,190:INFO: Epoch 1/4
